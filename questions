
Hvordan kan vi inarbeide reward i nettverket?

	https://github.com/Kautenja/nes-py/wiki/Creating-Environments

Hvordan designe reward funksjonen? (eks. reward for å oppdage nye brett?)

Kan vi bruke Q-learning/Q-matrix?

Hvordan er det best å designe CNN?

Hvor mange filtre bør vi ha?

Er observation_space lik høyde * bredde * 3 (RGB kanaler) = 240 * 256 * 3 = 184 320?
Eller høyde * bredde * 3 (RGB kanaler) * 255 = 47 001 600

Kan vi begrense observation_space hvis vi vet at enkelte deler er urelevante?

Kan vi lage "states" agenten kan være i (ie. exploration, attack, withraw etc.),
og evt. begrense action_space?
evt. state-spesifik Q-matrix (og/eller CNN)?

Kan vi legge inn relevant debugging info (fra env.step) i NN etter convolutional layers?

Hvordan best begrense observation_space -> descrete observation_space?

Output fra CNN -> discrete observation_space? Optimal CNN output size?

Kan vi begrense fargepalett til kun fargene som finnes i spillet?

	http://www.thealmightyguru.com/Games/Hacking/Wiki/index.php?title=NES_Palette
	https://www.zeldaclassic.com/wiki/index.php/Palette

Kan vi begrense fargepaletten videre ut ifra om Link er i overworld, dungeon etc.?

Bør vi ha en time_penalty?

I zelda_env.py, kan vi få tak i last_action?

Hvordan knytte sammen CNN og Q-Learning systemet?

	https://www.practicalai.io/teaching-a-neural-network-to-play-a-game-with-q-learning/

Hvordan sjekke om man er i pause menyen / hvordan sjekke hvilken "map square" man er på?

hvordan hente ut RAM-verdier fra nes_env i zelda_env?

  .-----------------------------------------------------------------------------.
  |	https://datacrystal.romhacking.net/wiki/The_Legend_of_Zelda:RAM_map	|
  '-----------------------------------------------------------------------------'

	https://www.nesmaps.com/maps/Zelda/ZeldaOverworldQ2.html

loitering_penalty: increase exponentially with time?

YOLO classification of map square --> derive target location --> reward = -dist(target, Link)

Pathfinding algorithm? --> A* (or aleph*)


----------------------------------------------------------------------------------------------------------------

If Hθ is the heuristic (i.e. the reward given a state-action pair),
why is Hθ(S(s)) described as being represented by a deep neural network predicting Q-values?

We need help translating this from julia into python:

# a Node contains the minimum needed to hold the tree structure
struct Node{ACTIONC}
    action_ix::Int32 # action index leading to this state
    parent::Nullable{Node{ACTIONC}}
    children::Dict{Int32, Node{ACTIONC}}
    id::Int32
    function Node(
                  action_ix::Int32,
                  parent::Nullable{Node{ACTIONC}},
                  id::Int32) where ACTIONC
        children = Dict{Int32, Node{ACTIONC}}()
        return new{ACTIONC}(action_ix, parent, children, id)
    end
end